tm_map(content_transformer(tolower)) %>%
tm_map(removeWords, stopwords("english"))
#build matrix with frequency
dtm <- TermDocumentMatrix(docs1)
matrix <-as.matrix(dtm)
words <- sort(rowSums(matrix), decreasing = T)
df <- data.frame(word = names(words), freq = words)
set.seed(1234)
wordcloud2(data = df,
size = 1,
gridSize = 15,
fontWeight = "normal"
)
library(dplyr)
text$length <- nchar(text$cargo_description)
text %>%
arrange(-length) %>%
slice_head(., n = 5) %>%
kableExtra::kbl()
## Reproducibility info
options(width = 120)
session_info()
#Create a vector containing only the text
deaths <- input %>%
data.table::fread(select = c("names", "sunk", "lives")) %>%
tibble()
View(deaths)
deaths$lives <- as.integer(deaths$lives)
View(deaths)
?kbl
setwd("../shipwrecks_wisconsin")
#get list of links
url <- "https://uslhs.org/national_archives/states_files/wisconsin.htm"
links <- read_html(url) #%>%
#unable to complete
library(rvest)
library(xml2)
#get list of links
url <- "https://uslhs.org/national_archives/states_files/wisconsin.htm"
links <- read_html(url) #%>%
links[[1]]
links[[2]]
links
links[2]
links
?read_html
print(links)
links <- read_html(url) %>%
html_nodes("p") %>%
html_text()
links[1:5]
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes("p") %>%
html_text()
links[1:5]
links <- read_html(url, options = c("NOBLANKS")) %>%
html_text()
links[1:5]
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes("p") %>%
html_text()
links[1:5]
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes("p") %>%
html_attrs("href")
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes("p") %>%
html_attrs(., "href")
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes("p") %>%
html_attr(., "href")
links[1:5]
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes("p") %>%
html_attrs("href")
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes("p") %>%
html_attrs()
links[1:5]
?rvest::html_attrs
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes("p") %>%
html_attr("href")
links
movie <- read_html("https://en.wikipedia.org/wiki/The_Lego_Movie")
movie
cast <- html_nodes(movie, "tr:nth-child(8) .plainlist a")
cast
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes("a") #%>%
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes(., "a") #%>%
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes("span a") #%>%
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes(., "span a") #%>%
#get list of links
url <- "https://uslhs.org/national_archives/states_files/wisconsin.htm"
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes(., "span a") #%>%
links[1]
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes(., "span a") %>%
html_attr("href")
links
links <- links[-1:11]
links[1:11] <- NULL
links[1:11] <- NA
links <- na.omit(links)
links
gsub("..", "https://uslhs.org/national_archives", links[1:5])
gsub("\\.\\.", "https://uslhs.org/national_archives", links[1:5])
links <- gsub("\\.\\.", "https://uslhs.org/national_archives", links)
links
links[1]
url1 <- links[1]
info <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes(., "p span") %>%
html_text()
url1 <- links[1]
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "p span") %>%
html_text()
info
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "p span") %>%
html_name()
info
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "p span") %>%
html_attr()
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "p span") %>%
html_text()
info
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "p span") %>%
html_text() %>%
gsub(" ", "", .)
info
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "p span") %>%
html_text() #%>%
info
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "div p") %>%
html_text() #%>%
info
?html_text
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "div p") %>%
html_text(trim = T) #%>%
info
is.na(info)
info <- info[info != ""]
info
start <- grep("DESCRIPTION OF LIGHT", info)
start
info
?extract
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "div p") %>%
html_text(trim = T) %>%
extract(. != "")
info
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "div p") %>%
html_text(trim = T) %>%
extract(. != "") %>%
extract(., grep("DESCRIPTION OF LIGHT", .))
info
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "div p") %>%
html_text(trim = T) %>%
extract(. != "") %>%
extract(., grep("DESCRIPTION OF LIGHT", .):tail(., 1))
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "div p") %>%
html_text(trim = T) %>%
extract(. != "") %>%
extract(., grep("DESCRIPTION OF LIGHT", .):length)
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "div p") %>%
html_text(trim = T) %>%
extract(. != "") %>%
extract(, grep("DESCRIPTION OF LIGHT", .):length(.))
info <- read_html(url1, options = c("NOBLANKS")) %>%
html_nodes(., "div p") %>%
html_text(trim = T) %>%
extract(. != "") %>%
extract(grep("DESCRIPTION OF LIGHT", .):length(.))
info
get_lightouse_description <- function(url){
read_html(url) %>%
html_nodes(., "div p") %>%
html_text(trim = T) %>%
extract(. != "") %>%
extract(grep("DESCRIPTION OF LIGHT", .):length(.))
}
desc <- lapply(links, get_lightouse_description)
get_lightouse_description <- function(url){
read_html(url) %>%
html_nodes(., "div p") %>%
html_text(trim = T) %>%
extract(. != "") %>%
extract(grep("DESCRIPTION OF LIGHT", .):length(.)) %>%
print(url)
}
desc <- lapply(links, get_lightouse_description)
get_lightouse_description <- function(url){
print(url) %>%
read_html(url) %>%
html_nodes(., "div p") %>%
html_text(trim = T) %>%
extract(. != "") %>%
extract(grep("DESCRIPTION OF LIGHT", .):length(.))
}
desc <- lapply(links, get_lightouse_description)
links
links <- links[-grep("states\\.htm", links)]
desc <- lapply(links, get_lightouse_description)
lapply(desc, "[[", 1)
lapply(desc, "[[", length)
lapply(desc, "[[", 10)
lapply(desc, "[[", 2)
lapply(desc, tail, 1)
links[1:13]
lapply(desc[1:13], tail, 1)
links
desc
sapply(desc, length)
table(sapply(desc, length))
table(sapply(desc, function(x){length(x)))
table(sapply(desc, function(x){length(x))
table(sapply(desc, function(x){length(x)})
)
table(sapply(desc, function(x){length(x)}))
sapply(desc, function(x){which(x == length(10))})
sapply(desc, function(x){which(length(x) == 10)})
unlist(sapply(desc, function(x){which(length(x) == 10)}))
lapply(desc, "[[", 2)
lapply(desc, function(x){tail(, 1)})
lapply(desc, function(x){tail(x, 1)})
grep("°", unlist(lapply(desc, function(x){tail(x, 1)})))
grep("°", unlist(lapply(desc, function(x){tail(x, 1)})), value = T)
grep("°|'", unlist(lapply(desc, function(x){tail(x, 1)})), value = T)
library(listviewer)
install.packages("listviewer")
desc
str(desc)
df <-
desc %>%
tibble(name = lapply(desc, "[[", 1)
)
df <-
desc %>%
tibble(name = lapply(desc, "[[", 2)
)
df <-
desc %>%
tibble(name = unlist(lapply(desc, "[[", 2))
)
unlist(lapply(desc, "[[", 2)
)
df <-
desc %>%
tibble(name = unlist(lapply(desc, "[[", 2))
)
df <-
desc %>%
tibble()
tibble(name = unlist(lapply(desc, "[[", 2))
df <-
desc %>%
tibble(name = unlist(lapply(desc, "[[", 2))
)
df
df <-
desc %>%
tibble(name = unlist(lapply(., "[[", 2))
)
df <- tibble(name = unlist(lapply(desc, "[[", 2)))
tail(1:10, 2)
length(1:10)
length(1:10) - 1
a <- 1:10length(1:10) - 1
a <- 1:10
a[length(a)-1]
df <- tibble(name = unlist(lapply(desc, "[[", 2)),
lat = unlist(lapply(desc, function(x){extract(length(x)-1)})))
df <- tibble(name = unlist(lapply(desc, "[[", 2)),
lat = unlist(lapply(desc, function(x){extract(length(x)-1)})))
lapply(desc, length)
lapply(desc, function(x){extract(x, length(x))
)
lapply(desc, function(x){extract(x, length(x))})
lapply(desc, function(x){extract(x, length(x)-1)})
df <- tibble(name = unlist(lapply(desc, "[[", 2)),
lat = unlist(lapply(desc, function(x){extract(x, length(x))}))
)
df <- tibble(name = unlist(lapply(desc, "[[", 2)),
lat = unlist(lapply(desc, function(x){extract(x, length(x))})),
lon = unlist(lapply(desc, function(x){extract(x, length(x)-1)}))
)
df
df <- tibble(name = unlist(lapply(desc, "[[", 2)),
lat = unlist(lapply(desc, function(x){extract(x, length(x)-1)})),
lon = unlist(lapply(desc, function(x){extract(x, length(x))}))
)
df
View(df)
missing <- c(4, 16, 19, 22, 23, 25, 29, 32, 33, 35, 42, 44, 45, 50, 51 53, 54)
missing <- c(4, 16, 19, 22, 23, 25, 29, 32, 33, 35, 42, 44, 45, 50, 51, 53, 54)
df <- df[-missing, ]
df$lat
gsub("°|'|\\"", "", df$lat
gsub("°|'|\"", "", df$lat
gsub("°|'|\"", "", df$lat)
gsub("°|'|\"", "", df$lat)
gsub("°|'|\"|[NW]", "", df$lat)
df <- tibble(name = unlist(lapply(desc, "[[", 2)),
lat = unlist(lapply(desc, function(x){extract(x, length(x)-1)})),
lon = unlist(lapply(desc, function(x){extract(x, length(x))}))
)
df
View(df)
missing
missing <- c(4, 13, 16, 19, 22, 23, 25, 28, 29, 32, 33, 35, 42, 44, 45, 50, 51, 54)
df
missing <- c(4, 13, 16, 19, 22, 23, 25, 28, 29, 32, 33, 35, 42, 44, 45, 50, 51, 54)
df <- df[-missing, ]
gsub("°|'|\"|[NW]", "", df$lat)
df <- tibble(name = unlist(lapply(desc, "[[", 2)),
lat = unlist(lapply(desc, function(x){extract(x, length(x)-1)})),
lon = unlist(lapply(desc, function(x){extract(x, length(x))}))
)
df
View(df)
df <- df[-missing, ]
print(df, n = nrow(df))
df <- tibble(name = unlist(lapply(desc, "[[", 2)),
lat = unlist(lapply(desc, function(x){extract(x, length(x)-1)})),
lon = unlist(lapply(desc, function(x){extract(x, length(x))}))
)
df
print(df, nrow(df))
print(df, nrow(df))
df <- tibble(name = unlist(lapply(desc, "[[", 2)),
lat = unlist(lapply(desc, function(x){extract(x, length(x)-1)})),
lon = unlist(lapply(desc, function(x){extract(x, length(x))}))
)
print(df, nrow(df))
tibble.print_max(df)
print(df, n = 56)
df <- tibble(name = unlist(lapply(desc, "[[", 2)),
lat = unlist(lapply(desc, function(x){extract(x, length(x)-1)})),
lon = unlist(lapply(desc, function(x){extract(x, length(x))}))
)
missing <- c(4, 13, 16, 19, 22, 23, 25, 28, 29, 32, 33, 35, 42, 44, 45, 50, 51, 53, 54)
df <- df[-missing, ]
print(df, n = 37)
gsub("°|'|\"|[NW]", "", df$lat)
str_trim(gsub("°|'|\"|[NW]", "", df$lat), side = "both")
stringr::str_trim(gsub("°|'|\"|[NW]", "", df$lat), side = "both")
clean_loc <- function(myStr){
stringr::str_trim(gsub("°|'|\"|[NW]", "", myStr), side = "both")
}
df[, c(2,3)] <- apply(df[, c(2, 3)], 2, clean_loc)
View(df)
page <- readLines(con = url)
url <- "https://uslhs.org/national_archives/ahnapee_front_files/premises.htm"
page <- readLines(con = url)
grep("built or established", page)
page[grep("built or established", page)]
page[grep("18", page)]
page[grep("1893", page)]
page[grep("[12]893", page)]
page[grep("[12][0-9]893", page)]
page[grep("[12][0-9]93", page)]
page[grep("[12][0-9][0-9][0-9]", page)]
page[grep("^[12][0-9][0-9][0-9] ", page)]
page[grep("[^12][0-9][0-9][0-9] ", page)]
page[grep("[^12][0-9][0-9][0-9]", page)]
page[grep("^[12][0-9][0-9][0-9]", page)]
page[grep("^[1|2][0-9][0-9][0-9]", page)]
page[grep("^[12][0-9][0-9][0-9]", page)]
page[grep("[12][0-9][0-9][0-9]", page)]
page[grep("1893", page)]
page[grep(">1893", page)]
page[grep("[12][0-9][0-9][0-9]<", page)]
page[grep(">[12][0-9][0-9][0-9]", page)]
page[grep(">[12][0-9][0-9][0-9]<", page)]
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes(., "span a") %>%
html_attr("href")
#get list of links
url <- "https://uslhs.org/national_archives/states_files/wisconsin.htm"
links <- read_html(url, options = c("NOBLANKS")) %>%
html_nodes(., "span a") %>%
html_attr("href")
#omit anything not a lighthouse
links[1:11] <- NA
links <- na.omit(links)
links <- links[-grep("states\\.htm", links)]
links
#"../ahnapee_rear.htm"
url <- "https://uslhs.org/national_archives/ahnapee_rear_files/premises.htm"
page <- readLines(con = url)
page[grep(">[12][0-9][0-9][0-9]<", page)]
a <-
url %>%
read_html() %>%
html_node("p span")
a <-
url %>%
read_html() %>%
html_node("p span") %>%
html_text()
a
#"../ahnapee_rear.htm"
url <- "https://uslhs.org/national_archives/ahnapee_rear_files/premises.htm"
a <-
url %>%
read_html() %>%
html_node("p span") %>%
html_text()
a <-
url %>%
read_html(.) %>%
html_node("p span") %>%
html_text()
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text()
a
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() %>%
extract(138)
a
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() %>%
extract(, 138)
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() %>%
extract()
a
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() %>%
extract("Store")
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() %>%
extract(1)
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() %>%
extract(2)
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() %>%
extract()
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() %>%
extract(grep("1893", .))
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() %>%
extract(grep("[12][0-9][0-9][0-9]", .))
a
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() #%>%
a
extract(136)
a <-
url %>%
read_html(.) %>%
html_nodes("p span") %>%
html_text() %>%
extract(136)
setwd("../cruise_ships")
list.files()
file.edit("README.Rmd")
setwd("../blogdown-default/")
serve_site()
