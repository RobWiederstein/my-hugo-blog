setwd("../tutorial_pca/")
## signs are random
require(graphics)
## the variances of the variables in the
## USArrests data vary by orders of magnitude, so scaling is appropriate
prcomp(USArrests)  # inappropriate
head(USArrests)
str(USArrests)
prcomp(USArrests, scale = TRUE)
prcomp(~ Murder + Assault + Rape, data = USArrests, scale = TRUE)
plot(prcomp(USArrests))
summary(prcomp(USArrests, scale = TRUE))
biplot(prcomp(USArrests, scale = TRUE))
# Compute PCA
res.pca <- prcomp(USArrests, scale = TRUE)
fviz_eig(res.pca)
fviz_pca_ind(res.pca,
col.ind = "cos2", # Color by the quality of representation
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
fviz_pca_var(res.pca,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
# Access to the PCA results
# Eigenvalues
eig.val <- get_eigenvalue(res.pca)
eig.val
# Results for Variables
res.var <- get_pca_var(res.pca)
res.var$coord          # Coordinates
res.var$contrib        # Contributions to the PCs
res.var$cos2           # Quality of representation
install.packages("caret")
library(caret)
vignette(package = "caret")
vignette("caret")
setwd("..")
dir.create("./tutorial_caret")
setwd("./tutorial_caret/")
file.create("./tutorial_01.R")
file.edit("./tutorial_01.R")
library(caret)
library(mlbench)
install.packages("mlbench")
?mlbench
library(mlbench)
?mlbench
library(caret)
library(mlbench)
data(Sonar)
library(caret)
library(mlbench)
data(Sonar)
View(Sonar)
set.seed(107)
inTrain <- createDataPartition(
y = Sonar$Class,
## the outcome data are needed
p = .75,
## The percentage of data in the
## training set
list = FALSE
)
View(inTrain)
208 * .75
## The output is a set of integers for the rows of Sonar
## that belong in the training set.
str(inTrain)
training <- Sonar[ inTrain,]
testing  <- Sonar[-inTrain,]
#partial least squares discriminant analysis (PLSDA) model
plsFit <- train(
Class ~ .,
data = training,
method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale")
)
#partial least squares discriminant analysis (PLSDA) model
plsFit <- train(
Class ~ .,
data = training,
method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale")
)
plsFit <- train(
Class ~ .,
data = training,
method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale")
## added:
tuneLength = 15,
## added:
trControl = ctrl
)
plsFit <- train(
Class ~ .,
data = training,
method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale"),
## added:
tuneLength = 15,
## added:
trControl = ctrl
)
ctrl <- trainControl(method = "repeatedcv", repeats = 3)
plsFit <- train(
Class ~ .,
data = training,
method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale"),
## added:
tuneLength = 15,
## added:
trControl = ctrl
)
ctrl <- trainControl(
method = "repeatedcv",
repeats = 3,
classProbs = TRUE,
summaryFunction = twoClassSummary
)
ctrl <- trainControl(
method = "repeatedcv",
repeats = 3,
classProbs = TRUE,
summaryFunction = twoClassSummary
)
plsFit <- train(
Class ~ .,
data = training,
method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale"),
## added:
tuneLength = 15,
## added:
trControl = ctrl
)
plsFit <- train(
Class ~ .,
data = training,
method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale"),
## added:
tuneLength = 15,
## added:
trControl = ctrl,
metric = "ROC"
)
plsFit
set.seed(123)
plsFit <- train(
Class ~ .,
data = training,
method = "pls",
## Center and scale the predictors for the training
## set and all future samples.
preProc = c("center", "scale"),
## added:
tuneLength = 15,
## added:
trControl = ctrl,
metric = "ROC"
)
plsFit
ggplot(plsFit)
plsClasses <- predict(plsFit, newdata = testing)
str(plsClasses)
head(plsProbs)
#The option type = "prob" can be used to compute class probabilities
#from the model
plsProbs <- predict(plsFit, newdata = testing, type = "prob")
head(plsProbs)
#a function to compute the confusion matrix and associated
#statistics for the model fit:
confusionMatrix(data = plsClasses, testing$Class)
##########################  FIT NEW MODEL ###########################
## To illustrate, a custom grid is used
rdaGrid = data.frame(gamma = (0:4)/4, lambda = 3/4)
rdaGrid
##########################  FIT NEW MODEL ###########################
## To illustrate, a custom grid is used
rdaGrid = data.frame(gamma = (0:4)/4, lambda = 3/4)
set.seed(123)
rdaFit <- train(
Class ~ .,
data = training,
method = "rda",
tuneGrid = rdaGrid,
trControl = ctrl,
metric = "ROC"
)
rdaFit <- train(
Class ~ .,
data = training,
method = "rda",
tuneGrid = rdaGrid,
trControl = ctrl,
metric = "ROC"
)
rdaFit
#How do these models compare in terms of their resampling results?
resamps <- resamples(list(
pls = plsFit,
rda = rdaFit
)
)
summary(resamps)
# Visualize Results
xyplot(resamps, what = "BlandAltman")
#for each resample, there are paired results a paired t-test can
#be used to assess whether there is a difference in the average
#resampled area under the ROC curve.
diffs <- diff(resamps)
summary(diffs)
?Sonar
?hclust
hc <- hclust(dist(USArrests), "ave")
plot(hc)
setwd("./aba-attrition")
setwd("..")
setwd("./aba-attrition/")
list.files()
load("./aba-consolidated-datsets")
load("./data_tidy/aba-consolidated-datsets")
list.files(full.names = T, path = "./data_tidy")
load("./data_tidy/aba-consolidated-datasets")
df <- combine_aba_datasets()
save(df, file = "./data_tidy/aba-consolidated-datasets")
save(df, file = "~/Dropbox/public/datasets/aba-consolidated-datasets")
source('~/Dropbox/coding/rproj/aba-attrition/R/build-consolidated-aba-db.R')
load("./Dropbox/public/datasets/aba-consolidated-datasets")
load("~/Dropbox/public/datasets/aba-consolidated-datasets")
# missing values and rescale
Amelia::missmap(df)
# Compute PCA
df[df == 0] <- NA
Amelia::missmap(df)
Amelia::missmap(df)
# Compute PCA
df[df == 0] <- NA
Amelia::missmap(df)
df[is.na(df)] <- 0
Amelia::missmap(df)
load("~/Dropbox/public/datasets/aba-consolidated-datasets")
# missing values and rescale
df[is.na(df)]
# missing values and rescale
length(df[is.na(df)])
# rescale
df.1 <- scale(df)
# rescale
df.1 <- scale(df[, 3:35])
df.qual <- df[, 1:2]
load("~/Dropbox/public/datasets/aba-consolidated-datasets")
# 11 missing values
df[is.na(df)] <- 0
load("~/Dropbox/public/datasets/aba-consolidated-datasets")
# 11 missing values
df[is.na(df)] <- 0
# rescale
df.quant <- scale(df[, 3:35])
df.qual <- df[, 1:2]
#hierarcahl clustering
tree <- hclustvar(xquant, xqual)
install.packages("ClustOfVar", dependencies = TRUE)
install.packages("cluster", dependencies = TRUE)
#hierarcahl clustering
tree <- hclustvar(xquant, xqual)
library('cluster')
library('ClustOfVar')
#hierarcahl clustering
tree <- hclustvar(df.quant, df.qual)
load("~/Dropbox/public/datasets/aba-consolidated-datasets")
library('ClustOfVar')
library('cluster')
# 11 missing values
df[is.na(df)] <- 0
# Divide into categorical and numerical
df.quant <- df[, 3:35])
df.qual <- df[, 1:2]
#hierarcahl clustering
tree <- hclustvar(df.quant, df.qual)
head(df.qual)
table(df.qual$school)
table(df.qual$type_of_school)
#hierarcahl clustering
tree <- hclustvar(df.quant)
plot(tree)
stab <- stability(tree, B=50)
plot(stab)
plot(stab)
dev.off()
plot(stab)
dev.off()
#stability plot ?stability
# Look at the peaks in the plot above. The maximum for stability is
#1.0, so the higer the peak, the better.
stab <- stability(tree, B=50)
plot(stab)
#Use the partition information for k-means clustering
k.means <- kmeansvar(df.quant, df.qual, init=30)
#Use the partition information for k-means clustering
k.means <- kmeansvar(df.quant, init=30)
summary(k.means)
#Use the partition information for k-means clustering
k.means <- kmeansvar(df.quant, init=3)
summary(k.means)
k.means$cluster # This produces a list of what variables are in which cluster.
?cluster::daisy
# Clustering respondents
# First, calculate distances
d <- daisy(df, metric = "gower")
# Clustering respondents
# First, calculate distances
d <- daisy(df.qual, metric = "gower")
# Clustering respondents
# First, calculate distances
d <- daisy(df.quant, metric = "gower")
# Second create dendrogram
fit <- hclust(d=d, method="complete")    # Also try: method="ward.D"
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=4)   # "k=" defines the number of clusters you are using
rect.hclust(fit, k=4, border="red") # draw dendogram with red borders around the 4 clusters
groups <- cutree(fit, k=5)   # "k=" defines the number of clusters you are using
groups <- cutree(fit, k=5)   # "k=" defines the number of clusters you are using
# Second create dendrogram
fit <- hclust(d=d, method="complete")    # Also try: method="ward.D"
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=5)   # "k=" defines the number of clusters you are using
dev.off()
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=4)   # "k=" defines the number of clusters you are using
rect.hclust(fit, k=4, border="red") # draw dendogram with red borders around the 4 clusters
groups <- cutree(fit, k=5)   # "k=" defines the number of clusters you are using
rect.hclust(fit, k=5, border="red") # draw dendogram with red borders around the 4 clusters
groups <- cutree(fit, k=5)   # "k=" defines the number of clusters you are using
rect.hclust(fit, k=5, border="red") # draw dendogram with red borders around the 4 clusters
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=5)   # "k=" defines the number of clusters you are using
rect.hclust(fit, k=5, border="red") # draw dendogram with red borders around the 4 clusters
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=6)   # "k=" defines the number of clusters you are using
rect.hclust(fit, k=6, border="red") # draw dendogram with red borders around the 4 clusters
#Produce a k-means plot
kfit <- kmeans(d, 4)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
#Produce a k-means plot
kfit <- kmeans(d, 6)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
df[142,1]
df[c(193,153,192,171,168,186,85,88),1]
df[c(193,153,192,171,168,186,85,88, 119, 37,122),1]
df[c(105,115,85,82,163),]
df[c(105,115,85,82,163),1]
df[128,1]
df[83,1]
df[163, 1]
df[69,1]
#factoextra
res.pca <- prcomp(df.quant, scale = TRUE)
library(factoextra)
fviz_eig(res.pca)
fviz_pca_ind(res.pca,
col.ind = "cos2", # Color by the quality of representation
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
fviz_pca_var(res.pca,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
df$lsat <- ggplot2::cut_interval(df$all50lsat, n = 2)
groups <- as.factor(df$type_of_school)
fviz_pca_ind(res.pca,
col.ind = df$lsat, # color by groups
palette = RColorBrewer::brewer.pal(2, "Set2"),
addEllipses = TRUE, # Concentration ellipses
ellipse.type = "confidence",
legend.title = "Groups",
repel = TRUE
)
# Access to the PCA results
# Eigenvalues
eig.val <- get_eigenvalue(res.pca)
eig.val
# Results for Variables
res.var <- get_pca_var(res.pca)
df$lsat <- ggplot2::cut_interval(df$all50lsat, n = 6)
groups <- as.factor(df$type_of_school)
fviz_pca_ind(res.pca,
col.ind = df$lsat, # color by groups
palette = RColorBrewer::brewer.pal(2, "Set2"),
addEllipses = TRUE, # Concentration ellipses
ellipse.type = "confidence",
legend.title = "Groups",
repel = TRUE
)
fviz_pca_ind(res.pca,
col.ind = df$lsat, # color by groups
palette = RColorBrewer::brewer.pal(6, "Set2"),
addEllipses = TRUE, # Concentration ellipses
ellipse.type = "confidence",
legend.title = "Groups",
repel = TRUE
)
setwd('../blogdown-default/')
blogdown:::new_post_addin()
setwd("../aba-attrition/")
load("~/Dropbox/public/datasets/aba-consolidated-datasets")
library('ClustOfVar')
library('cluster')
# 11 missing values
df[is.na(df)] <- 0
# Divide into categorical and numerical
df.quant <- df[, 3:35])
# Divide into categorical and numerical
df.quant <- df[, 3:35]
df.qual <- df[, 1:2]
#hierarcahl clustering
tree <- hclustvar(df.quant)
plot(tree)
dev.off()
#stability plot ?stability
# Look at the peaks in the plot above. The maximum for stability is
#1.0, so the higer the peak, the better.
stab <- stability(tree, B=50)
plot(stab)
dev.off()
#Use the partition information for k-means clustering
k.means <- kmeansvar(df.quant, init=3)
summary(k.means)
k.means$cluster # This produces a list of what variables are in which cluster.
# Clustering respondents
# First, calculate distances
d <- daisy(df.quant, metric = "gower")
# Second create dendrogram
fit <- hclust(d=d, method="complete")    # Also try: method="ward.D"
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=6)   # "k=" defines the number of clusters you are using
rect.hclust(fit, k=6, border="red") # draw dendogram with red borders around the 4 clusters
#Produce a k-means plot
kfit <- kmeans(d, 6)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
#factoextra
res.pca <- prcomp(df.quant, scale = TRUE)
library(factoextra)
fviz_eig(res.pca)
fviz_pca_ind(res.pca,
col.ind = "cos2", # Color by the quality of representation
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
fviz_pca_var(res.pca,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE     # Avoid text overlapping
)
df$lsat <- ggplot2::cut_interval(df$all50lsat, n = 6)
groups <- as.factor(df$type_of_school)
fviz_pca_ind(res.pca,
col.ind = df$lsat, # color by groups
palette = RColorBrewer::brewer.pal(6, "Set2"),
addEllipses = TRUE, # Concentration ellipses
ellipse.type = "confidence",
legend.title = "Groups",
repel = TRUE
)
# Access to the PCA results
# Eigenvalues
eig.val <- get_eigenvalue(res.pca)
eig.val
# Results for Variables
res.var <- get_pca_var(res.pca)
# or correlogram?
distance <- get_dist(df.quant)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
setwd("..")
dir.create("./trump-educ")
setwd("./blogdown-default/")
my.libraries
lapply(my.libraries, library, character.only = T)
serve_site()
file.copy(from = "./content/post/2021-05-12-2020-law-school-attrition-rates/feature.jpg", to = "./content/post/2021-05-17-2020-law-school-attrition-rates-part-2/")
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
citr:::insert_citation()
